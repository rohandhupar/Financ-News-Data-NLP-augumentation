{
  "cells": [
    {
      "metadata": {
        "_uuid": "1b8cec2c7aef1829614b05ba9b547939aa4cbc7d"
      },
      "cell_type": "markdown",
      "source": "# What this dataset is all about ?\n* comodities news articles from reuters , bloomberg website scrapped and other news blogs for financial news \n* each news has sentiment values from 2 to 10 \n* for confidiential reason dataset can't be done public\n* Task was to build financial sentiment classifier which can beat bloomberg company model so our baseline mark is 80% (bloomberg model) we have to cross this line "
    },
    {
      "metadata": {
        "_uuid": "95712e51df3cb9b3cefe6f2f7b316cb8f2630b2a"
      },
      "cell_type": "markdown",
      "source": "## Importing Libraries for necessary visualization of dataset and gensim for creating summary"
    },
    {
      "metadata": {
        "_uuid": "2cec013ab551feb2ffc620748003ea80c3ac68ab",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\nimport matplotlib_venn as venn\nfrom gensim.summarization.summarizer import summarize",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dacfcc51f2a81ca85ed16ac8b5bbcd382f909577"
      },
      "cell_type": "markdown",
      "source": "## These are datasets in which webscrapped data is there"
    },
    {
      "metadata": {
        "_uuid": "2390528b5971eddcc6d85373c5424585242548d8",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1=pd.read_csv('../input/commodities/commodities_feb_1k.csv',encoding=\"latin-1\")\ndf2=pd.read_csv('../input/commodities/commodities_feb_3.6k.csv',encoding=\"latin-1\")\ndf3=pd.read_csv('../input/commodities/commodities_feb_4.9k.csv',encoding=\"latin-1\")\ndf4=pd.read_csv('../input/commodities/commodities_jan_10k.csv',encoding=\"latin-1\")\ndf5=pd.read_csv('../input/commodities/commodities_jan_5k.csv',encoding=\"latin-1\")\ndf6=pd.read_csv('../input/commodities/commodities_jan_2k.csv',encoding=\"latin-1\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4497e44075ab73b760222335c9bdff89fcdd5dfe"
      },
      "cell_type": "markdown",
      "source": "## Concatenation and reshuffling the concatenated data"
    },
    {
      "metadata": {
        "_uuid": "b8cac6bce92e1cf33af9f91cdc370cced68f78b0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df4=pd.concat([df1,df2,df3,df5,df6,df4])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8e2ab41ce2b22c9d8e0f6e91e748e495a6cbf166",
        "trusted": true
      },
      "cell_type": "code",
      "source": "final_df= df4.sample(frac=1).reset_index(drop=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "351d8392099546f80f18041a32d1b9038f67fe86"
      },
      "cell_type": "markdown",
      "source": "## gensim needed for summarization "
    },
    {
      "metadata": {
        "_uuid": "19acaab735ec45f718945babba079a2d70349e86",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from gensim.summarization.textcleaner import split_sentences",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a1b1f649a4b83a33b9c9c4d423a7c57f0dafc2c2"
      },
      "cell_type": "markdown",
      "source": "## appostohphe list used for replacing the text , but  i did not  use it here for now"
    },
    {
      "metadata": {
        "_uuid": "df834ba9a1593cc5d01f21b9cbf54b9172e093e6",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#appos list \nappos = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\"\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "41cc5fd187c44f4a4ef2b1dd032a1b4fb183280b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "final_df=final_df.dropna()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "654dbbcac77cd1095ec9e6d1076f788a3de97647",
        "trusted": true
      },
      "cell_type": "code",
      "source": "final_df.isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "49b7550b7d9a3dd43a717e230c2d6d132961e218",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## basic pre-processing start down 1st spliting of text "
    },
    {
      "metadata": {
        "_uuid": "4288545d9a04d8db9f5f78358a9e7e0479f75217",
        "trusted": true
      },
      "cell_type": "code",
      "source": "final_df['Text'] = final_df['Text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\nfinal_df['Text'].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5c32560597630853e87f68df3418d7662dffd21b"
      },
      "cell_type": "markdown",
      "source": "## removal of punctuations "
    },
    {
      "metadata": {
        "_uuid": "71d76bde786abecaa72c5fb6d38daefcbaccf634",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# removing punctuation\nfinal_df['Text']=final_df['Text'].str.replace('[^\\w\\s]','')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bee1d7c5c0e5ed6dee6f3b74f99e89c9638cf8f7"
      },
      "cell_type": "markdown",
      "source": "## removing of stopwords"
    },
    {
      "metadata": {
        "_uuid": "c8b82f4be5fa30b2cc4bb9bb479b1ee80cd00625",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#removing stopwords\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfinal_df['Text']=final_df['Text'].apply(lambda x:\" \".join(x for x in x.split() if x not in stop))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c62ddf73e30587e62460cb007001afe2e5191cb8"
      },
      "cell_type": "markdown",
      "source": "## lemmatize is more effective than stemming using textblob \n## why lemmatize because , after lemmatize we will get ground value word rather than just if we do stemming is not so \n\n## Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n\n## Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma."
    },
    {
      "metadata": {
        "_uuid": "e305c02216fb98bcb0e9bdf377f3326b66dd53ff",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from textblob import Word\nfinal_df['Text'] = final_df['Text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "874b6741e074e94a2d0cca40abfc3e016598c87e"
      },
      "cell_type": "markdown",
      "source": "# basic data cleaning is done , the tokenization and padding can be done with keras before making deep learning models "
    },
    {
      "metadata": {
        "_uuid": "7980790d9366af33a80d4e946a7dec0e42d7776f"
      },
      "cell_type": "markdown",
      "source": "# Checking distribution of sentiment "
    },
    {
      "metadata": {
        "_uuid": "772613189f4d09625d447790b583e08f13da3a0b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "sns.countplot(final_df.Sentiment)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f205838fa387e7d40378247f83c2c7984a6bb63c"
      },
      "cell_type": "markdown",
      "source": "## From this we can see easily that there is distribution differences so we have to do binning of sentiment categories in 2 or 3 parts like 1 , 0 ,-1 or just 1 , 0 \n## 1 is positive and 0 is negative "
    },
    {
      "metadata": {
        "_uuid": "3e05286a4b8b87342e3619697fdf7daf3439b314",
        "trusted": true
      },
      "cell_type": "code",
      "source": "sns.distplot(final_df.Sentiment)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1bf3c29a521a6b62377f7a16ae1a3f1acdc3c2a6"
      },
      "cell_type": "markdown",
      "source": "## we can club upto three categories because as we can that 0 and 1 is very less and 9 and 10 to also very less \n## so we have idea to do binding upto three classes low , med and high \n\n## for low we can do binding from 2 to 4 level sentiment \n## for medium we can do binding from 5 to 7 level sentiment \n## for high we can do binding from 7 to 10 level sentiment "
    },
    {
      "metadata": {
        "_uuid": "15f514185cd557167b07283fad898046cff6097f",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(final_df[\"Text\"])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "12df171b2ab524c1da6dc26c9d19a530c28d7116"
      },
      "cell_type": "markdown",
      "source": "## in case of low relvance the most of the article with sentiment are around 2 to 5 "
    },
    {
      "metadata": {
        "_uuid": "b9104c3f1a38a303644908e187694d15a01b0eae"
      },
      "cell_type": "markdown",
      "source": "## So we have 304 rows with text from which full text has not been properly came up , if models does not work well , we will see this into our account "
    },
    {
      "metadata": {
        "_uuid": "0449a792cc8eb4b40d249c851801b8c5b5c95e52",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Most of the data cleaning part has been done and visualization has been also done , insight was bring down that we have to binding to 4 labels of sentiment \n*  highly negative for range of sentiment 2 to 3 \n*  moderate negative for range of sentiment from 4 to 5 \n* moderate postive for range of sentiment from 6 to 7 \n* highly positive for range of sentiment from 8 to 10 \n## More better we can bring the things to 3 classes like Postive , Negative , Neutral\n since we have balance our distribution for sentiments that's why we \n*  so 2 to 4 negative , 5 to 6 neutral , 7 to 10 postive sentiment , let's see a visualization \n## After observing  the results from neural nets i have changed to binary classification as or 0 and 1 "
    },
    {
      "metadata": {
        "_uuid": "60ceca9d2b78236a5b948c23e1f3a92a65175667",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#tempdf.loc[tempdf.num_hrefs <= 4.0,'num_hrefs']=1\nfinal_df.loc[(final_df.Sentiment <=10) & (final_df.Sentiment >=6 ),'Bins']=1\nfinal_df.loc[(final_df.Sentiment <=5) ,'Bins']=0",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "413869500ba5ac0eb1dee76f48cebde1f9eecb9a"
      },
      "cell_type": "code",
      "source": "final_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "30a8ecb1131fb40c4d0df80d15a099d84c440141"
      },
      "cell_type": "code",
      "source": "import re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport pandas as pd\nfrom nltk.corpus import stopwords\nimport re\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom bs4 import BeautifulSoup\ndef review_to_words( raw_review ):\n    # 1. Remove HTML\n    review_text = BeautifulSoup(raw_review, 'lxml').get_text() \n    \n    # 2. Remove non-letters with regex\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    \n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()                           \n    \n    # 4. Create set of stopwords\n    stops = set(stopwords.words(\"english\"))                  \n    \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stops]   \n    \n    # 6. Join the words back into one string separated by space, \n    # and return the result.\n    return( \" \".join( meaningful_words ))   \nfinal_df['Text']=final_df['Text'].apply(review_to_words)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b3e1f8dea2495646b5aa219c1e9862a93e200e75"
      },
      "cell_type": "markdown",
      "source": "## now we almost equal distributed data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a04d4a6dc6deb4fa3e1956dd0c6ae8c58b928e80"
      },
      "cell_type": "code",
      "source": "final_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "543c965850ba32d3cf0e039511c8a73766921490"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\npostive=final_df.loc[final_df['Bins'] == 1.0]\nnegative= final_df.loc[final_df[\"Bins\"]== 0.0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6bfb6e87cc4578dfa08b8b0256516ed443fc2cc8"
      },
      "cell_type": "markdown",
      "source": "## Data Augumentation in NLP \n* as we are provided with less data for long articles , it is indeed that we need more data to remove overfitting because to understand long article context and semantics learning data is needed atleast of 1 lakh rows , because i have done experimentation on dataset on kaggle on dataset of bag of words meet bag of popcorn, bring more data of movie review and used modified architecture i'm having most voted kernel on that dataset  \n\n## before augumentation and using hyper optimization we are with .7948 accuracy without overfitting at 7th epoch\n\n## our gradient was getting diverged by small decimal at each epoch and log loss was increasing with very very small steps so at last some tuning was applied and .7948 accuracy was bring down with out any overfitting \n\n## but after 7th epoch overfitting was started "
    },
    {
      "metadata": {
        "_uuid": "ea6f02d19a661b7874ed5a842dd4c1ede7525008"
      },
      "cell_type": "markdown",
      "source": "## While data augmentation is increasingly being used in machine learning to train models to classify images, when it comes to natural language processing (NLP) and text analytics, its application is still limited.\n\n## That’s partly because data augmentation is relatively new. But most importantly, it simply hasn’t been explored much in the text analytics space yet. It’s also less intuitive to do in NLP than it is with images.\n\n## What is data augmentation?\n\n* Let’s first consider the challenge of classifying specific images.\n\n* Say we want our model to be able to correctly classify images of cats and dogs, but we only have a limited number of labeled   images of cats and dogs. How can we create more to better train our model? And how can we make sure our model doesn’t just learn to recognize specific cats and dogs from among the few examples that we do have?\n\n* To create more images, we could modify slightly the existing ones. For example, we could flip an image of a dog horizontally, shift it slightly to the left or to the right, or zoom in or out. (See Exhibit 1.) We could also combine those modifications in various ways.\n\n## Text analytics: consider the message\n\n* Text, like a picture of a dog, cannot just be indiscriminately flipped and shifted.\n\n* If we did indiscriminately flip and/or shift it, we would end up with examples that would confuse our model, not make it smarter. Think of trying to train a model to classify movie reviews using both the review text and the same text written in reverse. It would make no sense.\n\n* So, while we can use data augmentation to get more text samples and improve our NLP models, we must first consider the message the text is conveying and the format in which it’s being presented.\n* Let’s take an example of a document with a specific format, like a résumé.\n A résumé is usually laid out in sections, such as personal information, work experience, education, interests, hobbies,\n \n* From a content perspective, those sections are independent from each other. In other words, each section would convey the same message and meaning even if the order in which they appeared in the document changed. If we shuffled the paragraphs in a résumé, we’d get another résumé conveying the same message.\n* resume has no format , anyone can write the any section at any position but that will change the meaning of resume independent sections \n* so shuffling section will not change the definition of resume same as with paragraph whether last is at first position or first is at last overall sentiment of article will be negative or positive \n\n\n## As i have done research on 4 techniques to bring augumentation in nlp \n\n## 1st is paragraph shuffling - shuffling will be done in such way we can new articles and no repeatation will be there \n\n## why shuffling will not affect context or sentiment of article\n   * The difference in the generated texts is based on the changed position of the sentences. As shuffling works as long as the context of the text is maintained. With sentence tokenization,the sentiment conveyed in the original text is still maintained. It would be lost if reviews were tokenized into words and then shuffled. Actually, although the meaning of a text may be lost due to words shuffling, it would not be an issue because feature extraction does consider order of words in a text.\n   * Wouldn’t just tokenizing a text into sentences and then shuffling lead to over-fitting? You may ask. That’s possible to some extend. Over-fitting can occur when there is a high variance in the sentiment vocabulary used in different reviews. The two reasons why over-fitting is less likely to occur as a result of augmentation used are:\n   * Based on what was observed from the negative sentiment examples that were used for augmentation, the vocabulary, or rather synonyms, does not seem to divert too much from phrases like:\n        falling of price\n        market closed at this price of stock \n        interest rate rising  ,\n        jump to a new position \n        gold prices rose etc\n\n\n\n## even neural nets make generalized pattern for indentifying postive or negative sentiment , shuffling the paragraph will not impose negative effect in generalization of pattern if context is not that much changed  , a pattern on which if you pass financial news article in anyway it will be able to detect that sentiment very accurately"
    },
    {
      "metadata": {
        "_uuid": "d12c0bf1b6f0689eafebb00e53f8cd65f45889f9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk import sent_tokenize\nimport random\ndef tokenize(text):\n    '''text: list of text documents'''\n    tokenized =  sent_tokenize(text)\n    return tokenized\n\ndef shuffle_tokenized(text):\n    random.shuffle(text)\n    newl=list(text)\n    shuffled.append(newl)\n    return text\n\naug_pos= []\naug_neg=[]\nreps=[]\nfor ng_rev in postive['Text']:\n    tok = tokenize(ng_rev)\n    shuffled= [tok]\n    #print(ng_rev)\n    for i in range(17):\n\n        shuffle_tokenized(shuffled[-1])\n    for k in shuffled:\n        '''create new review by joining the shuffled sentences'''\n        s = ' '\n        new_rev = s.join(k)\n        if new_rev not in aug_pos:\n            aug_pos.append(new_rev)\n        else:\n            reps.append(new_rev)\nfor ng_rev in negative['Text']:\n    tok = tokenize(ng_rev)\n    shuffled= [tok]\n    #print(ng_rev)\n    for i in range(17):\n\n        shuffle_tokenized(shuffled[-1])\n    for k in shuffled:\n        '''create new review by joining the shuffled sentences'''\n        s = ' '\n        new_rev = s.join(k)\n        if new_rev not in aug_neg:\n            aug_neg.append(new_rev)\n        else:\n            reps.append(new_rev)\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "59f518a6254afc62d13a7ff9df49478a919815f9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "new_positive=pd.DataFrame(data=aug_pos,columns=['Text'])\nnew_positive[\"Bins\"]=1.0\nnew_negative=pd.DataFrame(data=aug_neg, columns=[\"Text\"])\nnew_negative['Bins']=0.0",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6cdd42b1c3039cf8b7c79a63625e2d42ffd22c8e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "new_positive.shape ,new_negative.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "13ef3db39c4df8d125db18584128fa7cf843093d"
      },
      "cell_type": "markdown",
      "source": "## Synonyms augumentation is a technique where similar meaning words can be replaced , we are replacing every other word except Determiners and Proper Noun or Noun \n\n## Antonymns augumenation is a technique where opposite meaning of words can be replaced , for example , i love you become i hate you , we can change positive to negative and negative to positive sentences \n\n## Unfortunatly both the techniques were not giving good results reasons : \n* if i changed only adverb then data duplication went on higher rate chances were there \n* long articles this technique will never work , it good for news headlines sentiment or shorter text but not for long one \n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cbef6811b967b837d7f8d1051e1a766fc4d24399"
      },
      "cell_type": "code",
      "source": "#from nltk.corpus import wordnet\n#from nltk.tokenize import word_tokenize\n#from random import randint\n#import nltk.data\n\n# Load a text file if required\n#def syno_aug(text):\n #   output=\"\"\n  #  new_review = []\n\n# Load the pretrained neural net\n   # tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# Tokenize the text\n    #tokenized = tokenizer.tokenize(text)\n\n# Get the list of words from the entire text\n    #words = word_tokenize(text)\n\n# Identify the parts of speech\n    #tagged = nltk.pos_tag(words)\n\n    #for i in range(0,len(words)):\n        #replacements = []\n\n    # Only replace nouns with nouns, vowels with vowels etc.\n     #   for syn in wordnet.synsets(words[i]):\n\n        # Do not attempt to replace proper nouns or determiners\n      #      if tagged[i][1] =='NNP' or tagged[i][1] =='DT':\n        #        break\n       # \n        # The tokenizer returns strings like NNP, VBP etc\n        # but the wordnet synonyms has tags like .n.\n        # So we extract the first character from NNP ie n\n        # then we check if the dictionary word has a .n. or not \n         #   word_type = tagged[i][1][0].lower()\n          #  if syn.name().find(\".\"+word_type+\".\"):\n            # extract the word only\n           #     r = syn.name()[0:syn.name().find(\".\")]\n            #    replacements.append(r)\n        \n       # if len(replacements) > 0:\n            # Choose a random replacement\n        #    replacement = replacements[randint(0,len(replacements)-1)]\n         #   output = output + \" \" + replacement\n            \n        #else:\n            # If no replacement could be found, then just use the\n            # original word\n         #   output = output + \" \" + words[i]\n    #new_review.append(output)\n    #return(new_review)\n# Load a text file if required\n#def anto_aug(text):\n #   output=\"\"\n  #  new_review = []\n\n# Load the pretrained neural net\n   # tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# Tokenize the text\n    #tokenized = tokenizer.tokenize(text)\n\n# Get the list of words from the entire text\n    #words = word_tokenize(text)\n\n# Identify the parts of speech\n    #tagged = nltk.pos_tag(words)\n\n    #for i in range(0,len(words)):\n     #   replacements = []\n\n    # Only replace nouns with nouns, vowels with vowels etc.\n      #  for syn in wordnet.synsets(words[i]):\n       #     for lemma in syn.lemmas():\n        #        for antonym in lemma.antonyms():\n\n        # Do not attempt to replace proper nouns or determiners\n         #           if tagged[i][1] =='NNP' or tagged[i][1] =='DT':\n          #              break\n        \n        # The tokenizer returns strings like NNP, VBP etc\n        # but the wordnet synonyms has tags like .n.\n        # So we extract the first character from NNP ie n\n        # then we check if the dictionary word has a .n. or not \n           #         word_type = tagged[i][1][0].lower()\n            #        if antonym.name().find(\".\"+word_type+\".\"):\n            # extract the word only\n             #           r = antonym.name()[0:syn.name().find(\".\")]\n              #          replacements.append(r)\n        \n      #  if len(replacements) > 0:\n            # Choose a random replacement\n       #     replacement = replacements[randint(0,len(replacements)-1)]\n        #    output = output + \" \" + replacement\n            \n        #else:\n            # If no replacement could be found, then just use the\n        # original word\n         #   output = output + \" \" + words[i]\n #   new_review.append(output)\n  #  return(new_review)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4429ee97dbcd988c84b44e1a4c537470498caf43"
      },
      "cell_type": "markdown",
      "source": "# web scrapper and summary generator of articles for data augumenation "
    },
    {
      "metadata": {
        "_uuid": "30ca885a7ce63c7515e98e72b5dd2e1ac08cf753",
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "#import numpy as np\n#from newspaper import Article, ArticleException\n#import requests\n#links=[]\n#def text_extractor2(link):\n #   article = Article(link)\n  #  try:\n   #     article.download()\n    #    article.parse()\n     #   article = article.text[:]\n      #  text=summarize(article,ratio=0.7)\n    #except (ArticleException,ValueError):\n     #   print(link)\n      #  links.append(link)\n       # return np.nan\n   # return text\n    \n\n\n\n#final_df[\"summary\"]=final_df[15001:].URL.apply(text_extractor2)\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e9c129dc808b3a8bd9fadf2e7cf4defba99c34af"
      },
      "cell_type": "code",
      "source": "final_df.isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b6bc0111a8a8ae94241c0c97c34f4b243c7937ed"
      },
      "cell_type": "markdown",
      "source": "## importing summarize dataset with encoding latin-1 , concated them and reshuffling it "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "45017348b23267ac6b47a32b6ce59b3975f413b6"
      },
      "cell_type": "code",
      "source": "summary_df1=pd.read_csv('../input/summary-finance/summary_3314.csv',encoding='latin-1')\nsummary_df2=pd.read_csv('../input/summary-finance/summary_15001.csv',encoding='latin-1')\nsummary_df3=pd.read_csv('../input/summary-finance/summary_17191.csv',encoding='latin-1')\nsummary_df4=pd.read_csv('../input/summary-finance/summary_6000.csv',encoding='latin-1')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d191e41afeb7136db21082375bc5bbec0d1cf57e"
      },
      "cell_type": "markdown",
      "source": "# We have also used summarization augumentation it was leading good result at some epochs but not much greater than shuffling , it was tested on local system , so i have placed code here also so anyone can study this also use it in own purpose"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b56345f8cd6ffecb0903ba65a38b02c85ef8f027"
      },
      "cell_type": "code",
      "source": "summary=pd.concat([summary_df1,summary_df2,summary_df3,summary_df4])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0df904349db2a2ca549966d8b61b8152985140a2"
      },
      "cell_type": "code",
      "source": "summary= summary.sample(frac=1).reset_index(drop=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3c05bfcc4d42a22ef7f54120a249adffa86a5063"
      },
      "cell_type": "code",
      "source": "summary",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "84b80bb850bb7e37ba3ba50cd34ae9c31baa52d8"
      },
      "cell_type": "markdown",
      "source": "## Cleaning of summary needs to be done because article were coming from web "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e9e521d4ba6de8182a3981c392825b85004cf3a9"
      },
      "cell_type": "code",
      "source": "#summary['summary'] = summary['summary'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n#summary['summary']=summary['summary'].str.replace('[^\\w\\s]','')\n#summary['Text']=summary['Text'].apply(lambda x:\" \".join(x for x in x.split() if x not in stop))\n#freq = pd.Series(' '.join(summary['summary']).split()).value_counts()[:20] # removing top 20 common words\n#rare = pd.Series(' '.join(summary['summary']).split()).value_counts()[-20:] #removing top 20 rare words\n#freq=list(freq.index)\n#rare=list(rare.index)\n#summary['summary']=summary['summary'].apply(lambda x :\" \".join(x for x in x.split() if x not in freq))\n#summary['summary']=summary['summary'].apply(lambda x :\" \".join(x for x in x.split() if x not in rare))\n#summary['summary'] = summary['summary'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a6f35ae25007264e728d483067049f7b8919c581"
      },
      "cell_type": "code",
      "source": "#summary=summary.loc[:,['Bins','summary']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f72e7a626ed80e90d06001465d55f435282b2a2"
      },
      "cell_type": "code",
      "source": "#summary=summary.rename(columns={'Bins':'Target','summary':'Text'})\nnew_negative=new_negative.rename(columns={'Bins':'Target','Text':'Text'})\nnew_positive=new_positive.rename(columns={'Bins':'Target','Text':'Text'})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2756b3aacf602c1307f8a0d01e7bf241a156dce0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "sentences=pd.concat([new_positive,new_negative])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e86b90a6bc1acb1fe336bbb9575e6fb54d1ae0fa"
      },
      "cell_type": "markdown",
      "source": "## 1st we will see without augumentation what results are coming "
    },
    {
      "metadata": {
        "_uuid": "bc746528bd04a24fa174fd6442f654bf451f17ee",
        "trusted": true
      },
      "cell_type": "code",
      "source": "final_new = final_df.loc[:,['Text','Bins']]\nfinal_new=final_new.rename(columns={'Bins':'Target','Text':'Text'})\nfinal_update=pd.concat([final_new])\nfinal_update= final_update.sample(frac=1).reset_index(drop=True)\ntest_data=final_update.sample(3000)\nfinal_update=final_update.drop(test_data.index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b36f3085fdd16448ce00c59498837b115f467690"
      },
      "cell_type": "code",
      "source": "x = final_update.Text\ny = final_update.Target.astype(int)\nfinal_update.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4e068036509639be808cdf9d43c0fb816a0fc0d1"
      },
      "cell_type": "markdown",
      "source": " Background of the Techniques Convolution Neural Networks (CNN):\n\nCNN’s are efficient for sentence classification tasks as the convolution layers can extract features horizontally from multiple words . These characteristics are essential for classification tasks as it is tricky to find clues about class memberships especially when these clues can appear in different orders in the input. CNN has also been used for document topic classifications where a single local phrase could aid in establishing the topic regardless of the position where it appears in the document. They found that CNN is powerful enough to find these local indicators due to the powerful combination of the convolution and pooling layers. Long Short-Term Memory (LSTM):\n\nAn example of LSTM’s effectiveness is its ability to capture changing sentiment in a tweet. A sentence such as “The movie was fine but not to my expectation” contains words with conflicting sentiments which is not able to be inferred accurately by a typical neural network. However, LSTM will learn that the sentiments expressed towards the end of the sentence would carry more important context compared to the words at the start.\n\nCNN — LSTM Model:\n\nThe model architecture is . We initialized the model with Keras’ Sequential layer and added the embedding layer as the first layer. By using the embedding layer, the positive integers is turned into a dense vector of fixed size and this new representations will be passed to the CNN layer. Each filter in the CNN will detect specific features or patterns and then it will be pooled to a smaller dimension in the max-pooling layer. These features are then passed into a single LSTM layer of 100 units. Then, the LSTM outputs are then fed to a Fully Connected Layer (FCL) which was built using Keras’s Dense layer. As there are five labels to be predicted, a softmax activation function was used at the output layer."
    },
    {
      "metadata": {
        "_uuid": "ea23e119d2e52ecd3afefec963c66e9915f3b4d2"
      },
      "cell_type": "markdown",
      "source": "# Current Model"
    },
    {
      "metadata": {
        "_uuid": "f9fe6331e251798fc158a5510206ae370106a730",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from IPython.display import Image\nImage(\"../input/cnnlstm/System-architecture-of-the-proposed-regional-CNN-LSTM-model.png\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c2248e907c639c55a1a6b90ada7f8bb6e036a43d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dropout\nfrom keras.layers import MaxPool1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU,AveragePooling1D,MaxPooling1D\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.optimizers import Adam ,SGD\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation,GRU,Bidirectional\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.layers import Conv1D\nfrom keras.models import Model\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom keras import backend as K\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D,Flatten\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.layers.advanced_activations import LeakyReLU ,ELU ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8df9a022976423cc2ee0b551311e7010fc9b3e5b"
      },
      "cell_type": "markdown",
      "source": "## now we are good to proceed for model building "
    },
    {
      "metadata": {
        "_uuid": "f85d46da4df6ca4ede4b73a1eefc687182ef9dcc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "list_classes = [\"Target\"]\nlist_sentences_train = final_update[\"Text\"]\nlist_sentences_test = test_data[\"Text\"]\ny_test=test_data[\"Target\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6f2152f88148f8c8ae464547733680138b64ba41",
        "trusted": true
      },
      "cell_type": "code",
      "source": "max_features = 5000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fa7a9193240ea96941abe57e498b8c9fb6308613",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\ntotalNumWords = [len(one_comment) for one_comment in list_tokenized_train]\nplt.hist(totalNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\nplt.xlabel(\"Distribution of comment\")\nplt.ylabel(\"no of comments\")\nplt.title(\"no of comments vs no of words distribution \")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1df12623fe5a05e0c07ae9d199060d29a07091dc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "maxlen = 400\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te=pad_sequences(list_tokenized_test,maxlen=maxlen)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "872eaf10648857b91ac48799c92e1de4e679a2fc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "inp = Input(shape=(maxlen, ))\nembed_size = 128\nx = Embedding(max_features, embed_size)(inp)\nx = LSTM(98, activation='relu',return_sequences=True)(x)\nx = GlobalMaxPooling1D()(x)\nx = LeakyReLU(0.12)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\ncheckpointer = ModelCheckpoint(filepath=\"checkpoint.h5\", verbose=1, \n                              save_best_only=False)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', \n                              metrics=['accuracy'])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "93f57f7ee31cfd8ef7f9f4b4eff5207a9bb163f0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eb13a2aef7e4a4f40e4eedbbaac9fb8a1c2ed52d"
      },
      "cell_type": "markdown",
      "source": "# we will be stoping this model because we know this model will never work with this much data"
    },
    {
      "metadata": {
        "_uuid": "a78912b9b7cbacc94c300ee1c0dc5d0701341b9a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Start to train model \nhistory1 = model.fit(X_t, y, \n                    batch_size=32, \n                    epochs=1, \n                    verbose=1, \n                    validation_data=[X_te,y_test],\n                    callbacks=[checkpointer],\n                    shuffle=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cc4b3b166a3312c4881db073cef7043432ba26d7"
      },
      "cell_type": "markdown",
      "source": "# Since we can easily model is not making any improvement we have to stop the model"
    },
    {
      "metadata": {
        "_uuid": "7a7549b01d4bf80cee1341aee8bb2cc7da7bb208"
      },
      "cell_type": "markdown",
      "source": "## Now using augumented dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "648082a00887d8700aab7bfcafd9f3a13ae35268"
      },
      "cell_type": "code",
      "source": "final_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "164363d73b3302df84429ff1515e93914e85cea5"
      },
      "cell_type": "code",
      "source": "final_new = final_df.loc[:,['Text','Bins']]\nfinal_new=final_new.rename(columns={'Bins':'Target','Text':'Text'})\nfinal_update=pd.concat([final_new,new_positive,new_negative])\nfinal_update= final_update.sample(frac=1).reset_index(drop=True)\ntest_data=final_update.sample(3000)\nfinal_update=final_update.drop(test_data.index)\nx = final_update.Text\ny = final_update.Target.astype(int)\nfinal_update.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5ab1c023e492c78bf5b82ff5843aabad09abb5a9"
      },
      "cell_type": "markdown",
      "source": "# WILL ALWAYS BE TESTING ORIGINAL 3000 TEST CASES "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d68045bfcb76c6710ad444ab4a244245e66b90bf"
      },
      "cell_type": "code",
      "source": "list_classes = [\"Target\"]\nlist_sentences_train = final_update[\"Text\"]\nmax_features = 6000\nmaxlen = 400\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nlist_sentences_test=test_data[\"Text\"]\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\ny_test=test_data[\"Target\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d26f7954aecbd77890f757cce0b25ddef6f5be7c"
      },
      "cell_type": "code",
      "source": "inp = Input(shape=(maxlen, ))\nembed_size = 128\nx = Embedding(max_features, embed_size)(inp)\nx = Conv1D(64, 3, activation='relu')(x)\nx = Conv1D(128,3,activation='relu')(x)\nx = AveragePooling1D(pool_size=2)(x)\nx = Dropout(0.3)(x)\nx = LSTM(98, activation='relu',return_sequences=True)(x)\nx = GlobalMaxPooling1D()(x)\nx = ELU(0.2)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel1 = Model(inputs=inp, outputs=x)\ncheckpointer = ModelCheckpoint(filepath=\"checkpoint.h5\", verbose=1, \n                              save_best_only=False)\nmodel1.compile(loss='binary_crossentropy', optimizer='adam', \n                              metrics=['accuracy'])\nmodel1.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ad5bf39fafd8e726cfd8d60e3a9605ab6b772e41"
      },
      "cell_type": "code",
      "source": "# Start to train model \nhistory1 = model1.fit(X_t, y, \n                    batch_size=32, \n                    epochs=7, \n                    verbose=1, \n                    validation_data=[X_te,y_test],\n                    callbacks=[checkpointer],\n                    shuffle=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fb5b4416252b649d64184b5bca25ef41c0814eb4"
      },
      "cell_type": "markdown",
      "source": "# So after augumentation training and testing accuracy incrased but we can see overfitting also introduced \n# .9203 accuracy is highest we can achieve from the augumentation beaten the bloomberg model, now we are going for further new approach called N-gram multi channel CNN"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bebce2ffad4844d70f3d715c3b7181ffbac73ad3"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n%matplotlib inline\n# summarize history for accuracy\nplt.plot(history1.history['acc'])\nplt.plot(history1.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.grid(True)\nplt.legend(['train', 'test'], loc='upper left') \nplt.savefig(\"model_accuracy_100_salinas.svg\")\nplt.show()\n\n# summarize history for loss \nplt.plot(history1.history['loss'])\nplt.plot(history1.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.grid(True)\nplt.legend(['train', 'test'], loc='upper left') \nplt.savefig(\"model_loss_100_salinas.svg\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1a97a8616b63bbd97f7c29c8ddc93534f868c098"
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9333bc7cae84188bde82361e3b7bdd8a90481460"
      },
      "cell_type": "code",
      "source": "prediction = model1.predict(X_te)\ny_pred = (prediction > 0.5)\nprint(accuracy_score(y_pred,y_test))\nfrom sklearn.metrics import f1_score, confusion_matrix\nprint('F1-score: {0}'.format(f1_score(y_pred, y_test)))\nprint('Confusion matrix:')\nconfusion_matrix(y_pred, y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ac5ebcfa6e45fac6a309f493c87d68631247e6a1"
      },
      "cell_type": "markdown",
      "source": "## Another approach\nThe model can be expanded by using multiple parallel convolutional neural networks that read the source document using different kernel sizes. This, in effect, creates a multichannel convolutional neural network for text that reads text with different n-gram sizes (groups of words).\n\nDefine Model\nA standard model for document classification is to use an Embedding layer as input, followed by a one-dimensional convolutional neural network, pooling layer, and then a prediction output layer.\n\nThe kernel size in the convolutional layer defines the number of words to consider as the convolution is passed across the input text document, providing a grouping parameter.\n\nA multi-channel convolutional neural network for document classification involves using multiple versions of the standard model with different sized kernels. This allows the document to be processed at different resolutions or different n-grams (groups of words) at a time, whilst the model learns how to best integrate these interpretations.\n\nThis approach was first described by Yoon Kim in his 2014 paper titled “Convolutional Neural Networks for Sentence Classification.”"
    },
    {
      "metadata": {
        "_uuid": "5dae7b269584419c24a7780bf4e4291860041380",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def define_model(length,vocab_size):\n    # channel 1\n    inputs1 = Input(shape=(length,))\n    embedding1 = Embedding(vocab_size, 256)(inputs1)\n    conv1 = Conv1D(filters=32, kernel_size=5, activation='relu')(embedding1)\n    drop1 = Dropout(0.5)(conv1)\n    pool1 = MaxPooling1D(pool_size=2)(drop1)\n    flat1 = Flatten()(pool1)\n    # channel 2\n    inputs2 = Input(shape=(length,))\n    embedding2 = Embedding(vocab_size, 256)(inputs2)\n    conv2 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding2)\n    drop2 = Dropout(0.5)(conv2)\n    pool2 = MaxPooling1D(pool_size=2)(drop2)\n    flat2 = Flatten()(pool2)\n    # channel 3\n    inputs3 = Input(shape=(length,))\n    embedding3 = Embedding(vocab_size, 256)(inputs3)\n    conv3 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding3)\n    drop3 = Dropout(0.5)(conv3)\n    pool3 = MaxPooling1D(pool_size=2)(drop3)\n    flat3 = Flatten()(pool3)\n    # merge\n    merged = concatenate([flat1, flat2, flat3])\n    # interpretation\n    dense1= Dropout(0.3)(merged)\n    dense1 = Dense(50, activation='relu')(merged)  \n    outputs = Dense(1, activation='sigmoid')(dense1)\n    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n    # compile\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize\n    print(model.summary())\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f39badb1680935c7c4f7f4a57bbe34897f7518a"
      },
      "cell_type": "code",
      "source": "model = define_model(400, max_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "00df7044ec39025771fb79f39034a0e8c1b0f3ae"
      },
      "cell_type": "code",
      "source": "checkpointer = ModelCheckpoint(filepath=\"checkpoint.h5\", verbose=1, \n                              save_best_only=True)\nhistory1=model.fit([X_t,X_t,X_t], y,batch_size=32, \n                    epochs=10, \n                    verbose=1, \n                    validation_data=([X_te,X_te,X_te],[y_test]),\n                    callbacks=[checkpointer],\n                    shuffle=True)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6656d5919a3338b9c9e5900ab578521c02fd92be"
      },
      "cell_type": "code",
      "source": "# So we reached to the the highest accuracy with less log loss score ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "360b410cbc5d53178e798fc46772f8aa31bc3b19"
      },
      "cell_type": "code",
      "source": "# summarize history for accuracy\nplt.plot(history1.history['acc'])\nplt.plot(history1.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.grid(True)\nplt.legend(['train', 'test'], loc='upper left') \nplt.savefig(\"model_accuracy_100_salinas.svg\")\nplt.show()\n\n# summarize history for loss \nplt.plot(history1.history['loss'])\nplt.plot(history1.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.grid(True)\nplt.legend(['train', 'test'], loc='upper left') \nplt.savefig(\"model_loss_100_salinas.svg\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d34f57f293085ae74e6f8828f8e39c3038de0be"
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import accuracy_score ,classification_report",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "604d87e7d3d6c7a8d4f84a7d2576a96b39173ba5"
      },
      "cell_type": "code",
      "source": "preds = model.predict([X_te,X_te,X_te])\npreds = (preds[:,0] > 0.5).astype(np.int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9f643b150a9d2911955f4cfc000a3a3fefb9b0fd"
      },
      "cell_type": "code",
      "source": "print(accuracy_score(preds,y_test))\nfrom sklearn.metrics import f1_score, confusion_matrix\nprint('F1-score: {0}'.format(f1_score(preds, y_test)))\nprint('Confusion matrix:')\nconfusion_matrix(preds, y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7985543d110dd97d9d215c33bd88c034ecfc4b76"
      },
      "cell_type": "code",
      "source": "def model_predict(news):\n    list_sentences_train = news\n    max_features = 5000\n    maxlen = 400\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(list(list_sentences_train))\n    list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n    X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n    output=model.predict([X_t,X_t,X_t])\n    return output\n\n# postive news it is  # investment in tesla in electromotive cars\nnews=[\"\"\"nvestors interested in electric cars have a variety of options. Automakers such as Tesla Motors exclusively manufacture electric vehicles and may be directly invested in by purchasing stock. Companies within the automotive sector that manufacture vehicle parts or supply raw materials used in producing electric cars are another means of gaining portfolio exposure to electric cars. Another slightly less risky option is to invest in exchange-traded funds (ETFs) with holdings in securities related to electric vehicle production or electric vehicle parts.\n\nSome major automakers, such as Toyota, are investing heavily in electric vehicles and allow investors to choose both traditional and electric vehicles for their investments. Chevrolet and Nissan have also made notable electric car models available in the U.S. market. Investors should carefully consider available investment opportunities and evaluate the potential risk-return tradeoff offered by electric vehicles and the automotive industry.\n\nMany manufacturers develop auto parts for traditional and electric vehicles. Polypore International (PPO) produces lead-acid batteries used in both conventional and electric vehicles. This stock offers investors the opportunity to invest generally in the production of vehicle batteries. As electric vehicle and conventional vehicle usage grows, more batteries will be needed, and this company will likely benefit from increased global car demand.\n\nAnother battery company, Plug Power (PLUG), manufactures hydrogen fuel cell batteries used in electric vehicles and many other types of electronic equipment. These batteries may replace lead-acid batteries in fork lifts. Plug Power batteries are also used outside of the automotive industry, giving the company a large market.\n\nSociedad Quimica y Minera (SQM) is a major supplier of lithium, an element used in many batteries powering electric vehicles and other clean technologies. Investment in companies such as Polypore International, Plug Power, and SQM offers portfolio exposure to electric vehicles while also maintaining diverse holdings outside the automotive industry.\n\nElectric Vehicles Exchange-Traded Funds\nExchange-traded funds that track electric vehicles are another possible opportunity for investors. These funds allow investors to purchase shares in funds that track electric vehicle industry development. Investments are spread across multiple companies, reducing investment risk and offering returns similar to the average returns of the entire sector. ETFs track gains and losses of stock indexes and are traded directly on the stock market in a means similar to stock trading. Just as in traditional stock trading, stop-loss limits may be placed, and dividends are paid to brokerage accounts.\n\nSignificant ETFs that include electric vehicle stock and supplier stock include QCLN and LIT. The First Trust NASDAQ Clean Edge Green Energy Index Fund (QCLN) has Tesla among its holdings and includes other companies with green technology offerings. Global X Lithium (LIT) tracks lithium suppliers and battery companies. This fund's most significant holdings include FMC Corporation, Avalon Rare Metals Incorporated, and Rockwood.\n\"\"\"]\noutput=model_predict(news)\nprint(\"probability for this news\",output)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c931f32023bcf39f7e5698ccfcee19a2d05dc107"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e773bd05e0c691e5b1a739f5bfae8bb6b224124d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}